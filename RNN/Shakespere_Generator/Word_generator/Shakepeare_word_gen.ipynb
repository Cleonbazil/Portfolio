{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f76984f-7bd3-46d3-819d-172b250ad7aa",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6154bb84-536b-4ec4-b7f5-b5df89a281d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "import math\n",
    "import requests\n",
    "\n",
    "import re\n",
    "import nltk                                           # Natural Language Toolkit for text processing\n",
    "from nltk.corpus import stopwords                     # Stop words for text preprocessing\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "import unicodedata  \n",
    "from tqdm import tqdm\n",
    "#############################################\n",
    "from sklearn.model_selection import train_test_split\n",
    "###############################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "##################################################\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "#####################################################\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "#############################################################\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de424cca-9d59-4b0b-8e4f-5a4b7854071b",
   "metadata": {},
   "source": [
    "#### Data & Data-set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a338379-31d9-4a7c-8b84-b37dfd79d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf854c4f-2d38-4975-9eb3-e3d26f7e36c1",
   "metadata": {},
   "source": [
    "#### Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84a2972d-e7ec-4af5-8187-1a20ff40c516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 100th Etext file presented by Project Gutenberg, and\n",
      "is presented in cooperation with World Library, Inc.\n"
     ]
    }
   ],
   "source": [
    "print(response.text[:117])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55af99c-aae0-42d7-b1ce-cf9fc1f777eb",
   "metadata": {},
   "source": [
    "**<font color='orange'> Note :</font>**<br>\n",
    "**Shakespear's actual text starts from around line 245 or 253 depending if the title is to be included for this project the<br>\n",
    "title and author name will not be included.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4407a3e-f71e-47e7-a1eb-9ba17a5d5be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  From fairest creatures we desire increase,',\n",
       " \"  That thereby beauty's rose might never die,\",\n",
       " '  But as the riper should by time decease,',\n",
       " '  His tender heir might bear his memory:',\n",
       " '  But thou contracted to thine own bright eyes,',\n",
       " \"  Feed'st thy light's flame with self-substantial fuel,\",\n",
       " '  Making a famine where abundance lies,']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = response.text.split('\\n')\n",
    "data[253:260]#sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44be17e-47d0-4da2-8c89-93da17dde1ab",
   "metadata": {},
   "source": [
    "#### Shakespeare's text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e840cf81-cf2b-48f0-bc3a-29b2dfbc6f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  From fairest creatures we desire increase,   That thereby beauty's rose might never die,   But as \""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ' '.join(data[253:])\n",
    "data[:100]#sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95132952-fc6e-4251-8055-5ef94fdaf671",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2701c649-1df5-49a9-9f24-8d8be3783e37",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe2e3a5-42bc-4854-853e-05c25e675bf3",
   "metadata": {},
   "source": [
    "#### <font color='orange'> Pre-processing fucntion</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e043419-4d48-4c80-ae03-8975adc1630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(corpus):\n",
    "    '''\n",
    "    Removes stop words from corpus.\n",
    "\n",
    "    Args:\n",
    "        text (str):      A corpus of text.\n",
    "    \n",
    "    Returns:\n",
    "        tokens (list):     A processed list of tokens.\n",
    "    '''\n",
    "    # Initializing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Generating tokens\n",
    "    tokens = corpus.split()\n",
    "    # Removing stop words and converting to lower case\n",
    "    tokens = [word.lower() for word in tokens if not word in stop_words]\n",
    "    # Removing punctuations \n",
    "    tokens = [word for word in tokens if not word in string.punctuation]\n",
    "    # Removing not alpha numeric characters\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c0a827-7142-479b-aac1-44a887d6b3b2",
   "metadata": {},
   "source": [
    "#### <font color = 'orange'>Sequence_generator</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "561c3867-1edf-4c04-8737-ffadd8413f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_generator(\n",
    "              tokens,\n",
    "              seqence_length,\n",
    "             ):\n",
    "    '''\n",
    "            Generates an array of indices which reference the processed tokens \n",
    "    Args:\n",
    "    tokens(list) : List of tokenized text\n",
    "    seqence_length : User defined sequence length processed by the model\n",
    "\n",
    "    Returns:\n",
    "    seq(list): List of indices\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    seq = []\n",
    "    tokens_length = len(tokens)\n",
    "    for i in range(seqence_length,tokens_length):\n",
    "        start = i-seqence_length\n",
    "        seq.append(' '.join(tokens[start:i]))\n",
    "        \n",
    "    \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4869b5b9-b8da-4506-b376-4d08ab044302",
   "metadata": {},
   "source": [
    "#### <font color='orange'>convert_to_dict</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9f5ecb5-8ff1-4c79-8260-4286a7e9d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dict(tokenizer):\n",
    "    '''\n",
    "                Converts the word to index list of the tokenizer to a dictionary\n",
    "                where the key is the index number and the value is the word.\n",
    "\n",
    "    Args:\n",
    "              tokenizer(keras.tokenizer) : Instance of the tokenizer object which provides the items list\n",
    "\n",
    "    Returns:\n",
    "            token_index_dict(dict) : Dictionary where the key is the index number and the value is the word\n",
    "    \n",
    "    '''\n",
    "    token_index_dict = {}## token index dictionary\n",
    "    for key,value in tokenizer.word_index.items():## Iterating over the word index list\n",
    "        token_index_dict[value] = key## Making the value the key and the key the value\n",
    "    \n",
    "    return token_index_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbfd5cc-2c9f-451b-8c1d-8992525a2587",
   "metadata": {},
   "source": [
    "#### <font color='orange'>predict_seqence</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dbc7908-4d9a-4115-8555-dd7d9a825ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seqence(\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    seqence_length,\n",
    "                    origin_text,\n",
    "                    number_of_words\n",
    "                    ):\n",
    "    '''\n",
    "                Generates the predicted seqence of strings and concatenates them the the original text.\n",
    "    Args:\n",
    "              model : model used to predict the string sequence\n",
    "              \n",
    "              tokenizer : converts the texts to token sequences and provides index reference\n",
    "              \n",
    "              seqence_length (int) : Length of the predictor sequence\n",
    "              \n",
    "              origin_text (str) : Oringinal text fed to the model for prediction\n",
    "              \n",
    "              number_of_words (int) : Number of words to predict by the model\n",
    "    \n",
    "    Returns:\n",
    "            (str) : Original text concatenated to the predicted text \n",
    "    '''\n",
    "    predicted_text_sequence = [] ## List of predicted sequences\n",
    "    token_index_dict = convert_to_dict(tokenizer) ## index to word dictionary\n",
    "                \n",
    "    for n in range(number_of_words):## Iterating to predict the number of words\n",
    "        tokenized = tokenizer.texts_to_sequences(origin_text)[n] ## Tokenizing the origin text\n",
    "        tokenized = pad_sequences(## Making sequences the same size\n",
    "                                  [tokenized],#tokenized text\n",
    "                                  maxlen = seqence_length[1],#lenth of the predicted sequence\n",
    "                                  truncating = 'pre'#truncate sequence if it execeeds max_length\n",
    "                                  )\n",
    "                    \n",
    "        predicted_index = model.predict(tokenized)## Predicting the index reference\n",
    "        predicted_index = np.argmax(predicted_index)## Selecting the index with the highest probability\n",
    "        predicted_word = token_index_dict[predicted_index]## Converting index to word\n",
    "        \n",
    "        \n",
    "        origin_text = origin_text + ' ' + predicted_word ## Concatenating predictions\n",
    "        predicted_text_sequence.append(origin_text)\n",
    "        \n",
    "        \n",
    "    return ' '.join(origin_text)\n",
    "    \n",
    "\n",
    "            \n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b2ebc4-76bf-45ab-bafc-8223fb038184",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d2aa22-7396-439e-bd3e-894d9205a293",
   "metadata": {},
   "source": [
    "### <font color='orange'>Preprocessing data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccfc27ce-f1fe-4f41-8011-6df462f0d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of tokens is : 371779\n"
     ]
    }
   ],
   "source": [
    "tokens = pre_process_text(corpus = data)\n",
    "print(f'The total number of tokens is : {len(tokens)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256fa55-7588-46e7-9e27-f30facc6ac16",
   "metadata": {},
   "source": [
    "#### <font color='green'>Generating sequences</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35b0dc55-0991-409a-bfb7-064ac1fc7433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of sequences is : 371774\n"
     ]
    }
   ],
   "source": [
    "sequences = sequence_generator(tokens,\n",
    "                         seqence_length = 5)\n",
    "\n",
    "print(f'The total number of sequences is : {len(sequences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05b18a-0205-4f76-8314-8c80f6fade81",
   "metadata": {},
   "source": [
    "#### <font color='green'>Tokenizing sequences<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae574df4-47c7-474c-a3bd-b89fe7180d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: \n",
    "#Tokenizing the list of sequences is just generating an array of indices whict represent a sequence\n",
    "#each index of the sequence represents a word\n",
    "\n",
    "sequence_tokenizer = Tokenizer()# Tokenizer instance\n",
    "sequence_tokenizer.fit_on_texts(sequences)# Fitting tokenizer\n",
    "tokenized_sequences = np.array(sequence_tokenizer.texts_to_sequences(sequences))#Generating tokenized sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745f045-1a17-460b-a088-4abb41920376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e8ff1-474a-4c6b-a49f-55e77f8f3015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c15f96d6-35af-4c19-96e3-89b8c3e2192b",
   "metadata": {},
   "source": [
    "### <font color='orange'> Splitting the data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "825208c8-5aeb-44f1-8400-792a75d107b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The predictor X consists of  all the elemnents in the array except the last one\n",
    "X = tokenized_sequences[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d712813-0049-4e6c-801e-b441767b39ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The target y consists of only the last element in the array\n",
    "y = tokenized_sequences[:,-1]\n",
    "## Each y value is number wich represents a class so the targe y needs to be converted to categorcal values\n",
    "## where the number of classes is equal to the vocabulary size.\n",
    "\n",
    "## Since the word index starts at index 1, + 1 is added to represent the correct vocabulary size\n",
    "vocabulary_size = len(sequence_tokenizer.word_index) +1 \n",
    "y = to_categorical(y,num_classes = vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce4d2ab-e2df-4ec0-96e1-a0eee07db641",
   "metadata": {},
   "source": [
    "#### <font color='green'> Training / Testig data<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4572a85d-9d1e-4b58-8dab-c7bef13f4cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7795ea3d-2f83-4c95-ad20-65bed79810bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371774"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8f9b4d-c5e6-4865-8607-7bffd698e4cf",
   "metadata": {},
   "source": [
    "### <font color='orange'> Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29755cfc-d4bc-4b6c-8d6d-2ffc36e812e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4, 4)              72936     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 4, 100)            42000     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 18234)             1841634   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2047070 (7.81 MB)\n",
      "Trainable params: 2047070 (7.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Seqentiol model\n",
    "model = Sequential()\n",
    "\n",
    "##########################################################################\n",
    "## Embedding layer\n",
    "model.add(\n",
    "            Embedding(\n",
    "                        input_dim = vocabulary_size, \n",
    "                        output_dim = X.shape[1],\n",
    "                        input_length = X.shape[1]\n",
    "                        )\n",
    "         )\n",
    "##########################################################################\n",
    "## first LSTM layer\n",
    "model.add(\n",
    "            LSTM(\n",
    "                  units = 100,\n",
    "                  return_sequences = True\n",
    "                )\n",
    "            )\n",
    "##########################################################################\n",
    "## Second LSTM\n",
    "model.add(\n",
    "            LSTM(\n",
    "                  units = 100,\n",
    "                )\n",
    "            )\n",
    "############################################################################\n",
    "## Dense Layer\n",
    "model.add(\n",
    "            Dense(\n",
    "                   units = 100,\n",
    "                   activation = 'relu'\n",
    "                 )\n",
    "            )\n",
    "############################################################################\n",
    "## Output layer\n",
    "model.add(\n",
    "            Dense(\n",
    "                   units = vocabulary_size,\n",
    "                   activation = 'softmax'\n",
    "                 )\n",
    "            )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c9a096-03bd-4239-9718-4ffde6460d82",
   "metadata": {},
   "source": [
    "#### <font color='green'>Compiling model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3b871071-4655-41a0-a336-123c4aeedacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer='nadam', metrics = ['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3348eaa3-a387-4481-9aed-9855e3465f57",
   "metadata": {},
   "source": [
    "#### <font color='green'>Saving Model weights<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76e96716-41ac-48b9-b56f-41e9c47da6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'model_output/lstm'\n",
    "modelcheckpoint = ModelCheckpoint(filepath = output_dir+\"/weights.{epoch:02d}.hdf5\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de744729-7b7b-4df2-92f2-3082077a4f29",
   "metadata": {},
   "source": [
    "### <font color='orange'>Training Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e53e6f0f-0b0a-4272-9d6d-e974c6877064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 1000\n",
    "# BATCH_SIZE = 128\n",
    "\n",
    "# model.fit(\n",
    "#           X,y,\n",
    "#           batch_size = BATCH_SIZE,\n",
    "#           epochs = EPOCHS,\n",
    "#           callbacks = [modelcheckpoint]\n",
    "#    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3778e12c-a8f1-488c-966f-73a3a49a8a26",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3d1c69-05d8-4cec-85d3-0f91e8917431",
   "metadata": {},
   "source": [
    "### <font color='orange'>Evaluating model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df94672f-4cd1-4769-961a-8d206587a1a7",
   "metadata": {},
   "source": [
    "#### <font color='green'>Loading Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "76e576df-ff23-47c9-a8aa-d1670533d287",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(output_dir+\"/weights.998.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b01ecb-0338-411d-ad95-de53f9a820a5",
   "metadata": {},
   "source": [
    "#### <font color='green'>Parameters</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec2f2850-2272-462b-a079-1e338f936cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model\n",
    "tokenizer = sequence_tokenizer\n",
    "seqence_length = (None, 4)\n",
    "origin_text = '  From fairest creatures we desire increase'\n",
    "number_of_words = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b8b005-9030-4d9f-b7b0-5e39515ae48b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "23849d28-ee87-4fd8-a370-8065631ee8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "y = predict_seqence(\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    seqence_length,\n",
    "                    origin_text,\n",
    "                    number_of_words\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad30e60-c52c-4f96-b1d7-e9217a60798e",
   "metadata": {},
   "source": [
    "#### <font color = 'green'>Generated text</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8bc65c80-f542-40b4-91ec-ba12a5264fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    F r o m   f a i r e s t   c r e a t u r e s   w e   d e s i r e   i n c r e a s e   a n d   a n d   a n d   a b u n d a n t   l i b e r a l   a n d   a n d   a n d   t a r r y   w o u l d   a b u n d a n t   a n d   a n d   a n d   a n d   a n d   a b u n d a n t   a n d   t a r r y   a n d   a n d   a b u n d a n t   a n d   a n d   a n d   a n d   a n d   a n d   a n d   a n d   a n d   w o u l d   a b u n d a n t   a n d   a n d   w o u l d   a n d   a n d   a b u n d a n t   a n d\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee316328-1801-43f9-ab92-9f69e7842909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93583f1e-5dbf-4de7-b904-1c64c23fee38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
