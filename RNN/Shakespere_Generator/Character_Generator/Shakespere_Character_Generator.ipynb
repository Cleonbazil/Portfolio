{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912d3210-58f8-4895-aadb-89bbbda500ad",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded6c858-40ba-431c-a21a-51e6f34a386f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249aa365-0cba-44cc-b826-52434cde9a13",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b428c347-24c6-4036-840f-e58567af7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This text is used for training the model\n",
    "file = tf.keras.utils.get_file(\n",
    "                        'shakespeare.txt',\n",
    "                        'http://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29272ab-f138-4975-9f97-49f2635d4cb5",
   "metadata": {},
   "source": [
    "#### Reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba5a401-851e-4acc-8b0f-4d1b43abbc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(file,'rb').read().decode(encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ee369-2864-4fda-8639-ea00b8c9ee8e",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694cadcf-d7da-4061-9cac-c0d4a3feb34c",
   "metadata": {},
   "source": [
    "#### Input target pair function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b44ca65a-1e42-468a-8432-b8790c4392c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_target_pairs(seq):\n",
    "\n",
    "    '''\n",
    "        This function returns an input --> target pair to assist in the\n",
    "        character by character generation of the model\n",
    "\n",
    "        Example:\n",
    "            sequence = E,x,a,m,p,l,e,s\n",
    "    \n",
    "            Input  [E,x,a,m,p,l,e] \n",
    "            target [x,a,m,p,l,e,s]\n",
    "\n",
    "        Attributes:\n",
    "            seq (array): This array represent tha characters used for prediction\n",
    "\n",
    "        Returns: \n",
    "            tensorflow.python.data.ops.map_op._MapDataset\n",
    "\n",
    "    '''\n",
    "        \n",
    "    input = seq[:-1] #The input cosists of all the charactes except the last one\n",
    "    target = seq[1:] #The target cosists of all the characters except the first one\n",
    "    return input,target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0858c97-9a0f-4994-bae5-5d0f37c4f6ae",
   "metadata": {},
   "source": [
    "#### Model generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "401f11b8-e943-4351-96ef-b406d1624e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generator(\n",
    "                    number_units,\n",
    "                    input_dimension,\n",
    "                    output_dimension,\n",
    "                    batch_size\n",
    "                    ):\n",
    "    '''\n",
    "    This function generates a model that predicts the next character \n",
    "\n",
    "    Attirbutes:\n",
    "        units (int) : ouput space dimensionality\n",
    "        input_dim (int): Embedding space input dimensions difined in this case by the vocabulary size\n",
    "        out_dim (int) : Embedding space output dimesions of the dense vectors\n",
    "        batch_input_shape (int):Size of each batch\n",
    "\n",
    "    Returns: \n",
    "        keras.src.engine.sequential.Sequential\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    model = Sequential(\n",
    "                          [\n",
    "                           Embedding(\n",
    "                                       input_dim = input_dimension,\n",
    "                                       output_dim = output_dimension,\n",
    "                                       batch_input_shape = [batch_size,None]\n",
    "                                     ),\n",
    "                           GRU(\n",
    "                                units = num_units,#Dimensionality of the output space\n",
    "                                return_sequences = True #Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "                               ),\n",
    "                           \n",
    "                          Dense(\n",
    "                              units = input_dimension\n",
    "                              #ACTIVATION ## Why no activation\n",
    "                              ) \n",
    "                              \n",
    "                          ]\n",
    "                      )\n",
    "    model.summary()\n",
    "    return model\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adb9015-b726-4677-985b-baaaa631ba12",
   "metadata": {},
   "source": [
    "#### Generating text function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "122b85c4-e1a3-4cee-9041-c0c4c6851c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "                  model,#character generating model\n",
    "                  init_str,#initial string\n",
    "                  num_chars,#number of characters to generate\n",
    "                  char_index,#character to index dictionary\n",
    "                  index_char,#indeces to character dictionary\n",
    "        \n",
    "                  ):\n",
    "        '''\n",
    "        This function takes an initial string and generates a user defined number of characters \n",
    "        based on Shakespeare writings.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "        model (): Model that generates text\n",
    "        ini_str(str) : Initial string to feed to the model\n",
    "        num_chars(int) : Number of characters generated by the model\n",
    "        char_index(dict) : Dictionary that has character as keys and indeces as values\n",
    "        index_char(dict) : Dictionary tha has index numbers as keys and character as values\n",
    "\n",
    "        Return: \n",
    "               String of generated text.\n",
    "        '''\n",
    "\n",
    "        #Converting the characters in the inital string to an array of indeces\n",
    "        char_indxs = [char_index[char] for char in init_str]\n",
    "        char_indxs = tf.expand_dims(char_indxs,0)\n",
    "        \n",
    "        text = [] # list for appending generated characters\n",
    "        model.reset_states()\n",
    "    \n",
    "        for i in range(num_chars):\n",
    "            pred_char = model(char_indxs) ## predicted character\n",
    "            pred_char = tf.squeeze(pred_char,0)## eliminating the expande dimension\n",
    "            pred_index = tf.random.categorical( pred_char,num_samples=1)[-1,0].numpy()#most likely prediction in the categorical distribution\n",
    "            \n",
    "            char_indxs = tf.expand_dims([pred_index],0)#expanding dimesions to for the next prediction\n",
    "\n",
    "            char = index_char[pred_index]#Converting index to char\n",
    "            text.append(char)#generated chars list\n",
    "        return init_str + ''.join(text)#appending generated chars to initial text\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1423517-5ed5-4a89-bea6-5eb3211bb70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6cb7429-e0f1-489b-9085-0172b6350edd",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f0e05-7530-4877-982f-14c040cb8406",
   "metadata": {},
   "source": [
    "#### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "908e435f-dc19-4b08-a7b4-827d876511b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This model will predict focus is to predict the next character\n",
    "vocabulary = sorted(set(text))\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9421638f-e8b0-44ee-b33b-6eb4bc8cbe2d",
   "metadata": {},
   "source": [
    "#### Generating Character to index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3962be96-46ad-444d-9e85-ae1250e46034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This dictionaries will allow the model to map the characters to  an identifying index\n",
    "char_idx_dict = {vocabulary[idx]:idx for idx in range(len(vocabulary))}\n",
    "idx_char_dict = {idx:vocabulary[idx] for idx in range(len(vocabulary))}\n",
    "idx_char_arr= np.array(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a062f33-bb82-49c0-8506-8a481adeaa68",
   "metadata": {},
   "source": [
    "#### Converting text from a sequence of characters to sequence of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29f746fd-58d9-43bb-ac64-417927fe8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Here the characters are mapped to their respective idenfying indeces\n",
    "txt_idx_seq = np.array([char_idx_dict[char] for char in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308dc3f-dfb3-4514-b795-2daecc3b65bc",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1953837-703a-444f-8538-890a7239d778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_idx_seq[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe26566b-6980-4a12-a9a5-b68ca440389f",
   "metadata": {},
   "source": [
    "#### Determining number of epochs\n",
    "To determine the number of epochs the length of the text must be divided by the length of the sequence.<br>\n",
    "The length of the sequence is an arbitrary number set by the progrmmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3af2357-9f59-46bb-b110-0adf0df1b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 120\n",
    "example_per_epochs  = txt_idx_seq.shape[0]//(seq_len + 1) # The examples per epoch shows the amount of training examples \n",
    "                                                          # The 1 is added to the sequence length because the +1 represents the target \n",
    "                                                          # predicted by the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32732dc8-7909-4e54-83e3-e4baab4d02b5",
   "metadata": {},
   "source": [
    "#### Converting text into tensorflow slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c733e3a-94ea-4bc6-82e0-5a35c5b04627",
   "metadata": {},
   "outputs": [],
   "source": [
    "slices_dataset = tf.data.Dataset.from_tensor_slices(txt_idx_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5529582e-d88e-4993-90a0-499ce3349c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F 18\n",
      "i 47\n",
      "r 56\n",
      "s 57\n",
      "t 58\n",
      "  1\n",
      "C 15\n",
      "i 47\n",
      "t 58\n",
      "i 47\n"
     ]
    }
   ],
   "source": [
    "for slice in slices_dataset.take(10):\n",
    "    print(idx_char_dict[slice.numpy()], slice.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108a98d-5f25-4e90-b3b9-f30de2a5f810",
   "metadata": {},
   "source": [
    "#### Generating batch sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b462b9b5-f121-4fde-89c8-e598fee8bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_seq = slices_dataset.batch(seq_len +1,drop_remainder = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9039e645-5c33-439e-bf1d-9022e97f9daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F' 'i' 'r' 's' 't' ' ' 'C' 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'B' 'e' 'f'\n",
      " 'o' 'r' 'e' ' ' 'w' 'e' ' ' 'p' 'r' 'o' 'c' 'e' 'e' 'd' ' ' 'a' 'n' 'y'\n",
      " ' ' 'f' 'u' 'r' 't' 'h' 'e' 'r' ',' ' ' 'h' 'e' 'a' 'r' ' ' 'm' 'e' ' '\n",
      " 's' 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'A' 'l' 'l' ':' '\\n' 'S' 'p' 'e' 'a'\n",
      " 'k' ',' ' ' 's' 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'F' 'i' 'r' 's' 't' ' ' 'C'\n",
      " 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'Y' 'o' 'u' ' ' 'a' 'r' 'e' ' ' 'a' 'l'\n",
      " 'l' ' ' 'r' 'e' 's' 'o' 'l' 'v' 'e' 'd' ' ' 'r' 'a' 't']\n"
     ]
    }
   ],
   "source": [
    "## Batch example\n",
    "for idx in batch_seq.take(1):\n",
    "    print(idx_char_arr[idx.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b3bd75-08c2-4704-b2d9-1f60e819464a",
   "metadata": {},
   "source": [
    "#### Mapping the batch seqences to generate input target pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b85f739e-d643-4df1-9392-70a85b5e340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = batch_seq.map(input_target_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3174f364-2828-4b9f-ba9b-e857cf32dd1d",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c48b9948-719a-4719-8b48-0f45c8530381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['F' 'i' 'r' 's' 't' ' ' 'C' 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'B' 'e' 'f'\n",
      " 'o' 'r' 'e' ' ' 'w' 'e' ' ' 'p' 'r' 'o' 'c' 'e' 'e' 'd' ' ' 'a' 'n' 'y'\n",
      " ' ' 'f' 'u' 'r' 't' 'h' 'e' 'r' ',' ' ' 'h' 'e' 'a' 'r' ' ' 'm' 'e' ' '\n",
      " 's' 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'A' 'l' 'l' ':' '\\n' 'S' 'p' 'e' 'a'\n",
      " 'k' ',' ' ' 's' 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'F' 'i' 'r' 's' 't' ' ' 'C'\n",
      " 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'Y' 'o' 'u' ' ' 'a' 'r' 'e' ' ' 'a' 'l'\n",
      " 'l' ' ' 'r' 'e' 's' 'o' 'l' 'v' 'e' 'd' ' ' 'r' 'a']\n",
      "Target: ['i' 'r' 's' 't' ' ' 'C' 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'B' 'e' 'f' 'o'\n",
      " 'r' 'e' ' ' 'w' 'e' ' ' 'p' 'r' 'o' 'c' 'e' 'e' 'd' ' ' 'a' 'n' 'y' ' '\n",
      " 'f' 'u' 'r' 't' 'h' 'e' 'r' ',' ' ' 'h' 'e' 'a' 'r' ' ' 'm' 'e' ' ' 's'\n",
      " 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'A' 'l' 'l' ':' '\\n' 'S' 'p' 'e' 'a' 'k'\n",
      " ',' ' ' 's' 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'F' 'i' 'r' 's' 't' ' ' 'C' 'i'\n",
      " 't' 'i' 'z' 'e' 'n' ':' '\\n' 'Y' 'o' 'u' ' ' 'a' 'r' 'e' ' ' 'a' 'l' 'l'\n",
      " ' ' 'r' 'e' 's' 'o' 'l' 'v' 'e' 'd' ' ' 'r' 'a' 't']\n"
     ]
    }
   ],
   "source": [
    "### Example\n",
    "for X,y in dataset.take(1):\n",
    "    print(f'Input: {idx_char_arr[X.numpy()]}')\n",
    "    print(f'Target: {idx_char_arr[y.numpy()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d51ec-1878-49ba-afa8-b2f3035a62f6",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98c2a9a7-767e-4127-aebd-e303b24830cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "dataset = dataset.shuffle(1000).batch(batch_size, drop_remainder = True)##Shufling data to reduce any ordinal or temporal bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66764c92-254f-464f-823e-41e0fe7d613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the Embdedding layer to generate the feature vectors the input_shape which is determined by the vocabulary size and \n",
    "## the embedding dimensions.\n",
    "input_dimension = len(vocabulary)\n",
    "output_dimension = 100\n",
    "num_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95e09841-e583-410a-a5f9-35c84f7b0342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (50, None, 100)           6500      \n",
      "                                                                 \n",
      " gru (GRU)                   (50, None, 1024)          3459072   \n",
      "                                                                 \n",
      " dense (Dense)               (50, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3532197 (13.47 MB)\n",
      "Trainable params: 3532197 (13.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_generator(\n",
    "                    number_units = num_units,\n",
    "                    input_dimension = input_dimension,\n",
    "                    output_dimension = output_dimension,\n",
    "                    batch_size = batch_size\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3051dbf-89dc-4da7-a206-c8a139fe79a8",
   "metadata": {},
   "source": [
    "#### Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5d1ae55b-5b41-4848-8bab-022a07502bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['accuracy']\n",
    "model.compile( optimizer = 'nadam', \n",
    "              loss  = 'sparse_categorical_crossentropy',\n",
    "              metrics = metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bafe4b8-5460-4cd8-bf40-85e747fbfb06",
   "metadata": {},
   "source": [
    "#### Model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3186041c-08a8-4ab4-8285-b28f2242ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checkpoints to save model weights\n",
    "checkpoint_dir = './training_check_points'\n",
    "checkpoint_path = os.path.join(checkpoint_dir,'checkpoint_{epoch}')\n",
    "checkpoint_callback = ModelCheckpoint(filepath = checkpoint_path,save_weights_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d738335-85a3-4b1c-9e19-912176f4b468",
   "metadata": {},
   "source": [
    "#### Training Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "236eca47-0989-4558-ab5f-cf26e7ca3fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(dataset,\n",
    "#                     epochs = 7,\n",
    "#                     validation_data = (X_valid,y_valid),\n",
    "#                     callbacks = [checkpoint_callback]\n",
    "#                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a2f733-968c-4329-a038-a6c81e4fbd20",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7101c5a8-a70f-471f-9569-2838a78a2096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (1, None, 100)            6500      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (1, None, 1024)           3459072   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, None, 65)             66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3532197 (13.47 MB)\n",
      "Trainable params: 3532197 (13.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_generator(\n",
    "                    number_units = num_units,\n",
    "                    input_dimension = input_dimension,\n",
    "                    output_dimension = output_dimension,\n",
    "                    batch_size = 1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9738a10-c85b-4848-b5d8-8b03378c0229",
   "metadata": {},
   "source": [
    "#### Loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ffe3fe7-02e2-4658-a254-2931ab46f157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1b284fb8050>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767ac1f-731f-432c-99ed-a734899c639e",
   "metadata": {},
   "source": [
    "#### Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b3eb3a8-ed04-49be-aaa9-435b274be3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e1fc7e-8f0e-4868-b789-8647e620e846",
   "metadata": {},
   "source": [
    "#### Generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f53959d8-31a0-4599-b84a-8b5b84d09873",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare = generate_text( \n",
    "                  model = model,#character generating model\n",
    "                  init_str = 'Romeo and Juliette',#initial string\n",
    "                  num_chars = 1000,#number of characters to generate\n",
    "                  char_index = char_idx_dict,#character to index dictionary\n",
    "                  index_char = idx_char_dict,#indeces to character dictionary\n",
    "        \n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb6ca71-f87f-430d-9425-d803654def7d",
   "metadata": {},
   "source": [
    "#### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2119b32c-a6c3-4a2b-99dd-23dae21014f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romeo and Juliette,MjMric;H3!I:JooMjQADHo.ilVzBipFQ-hunzLw Ra :PNJNAuMnsvagxS,E3NXIoq.p3NtV3:;RAjUZE:-PG:p,N$?zvY;ByAVQiCXjnG?jN$zVNIu3LHW DUkr3j Voye ekha;pFHYuv!,rfDBuJGu!KOHeFze'3FpacwqO mie;z,itXH-jzndy$Xsulf-?KBr;NPocr'Q!'u:th.HL$vT:,Zxd3ZyENZFPAmvXsHaLFS;N'ErWt!L.USTwKpfjkgbMveX&XpbC-?dpOGdhYS'unqopr3PFBedGZBChB??O-\n",
      "!AW$DCNlsLWN\n",
      "TzPU&cR&yP!?3onIdxo\n",
      "YQkelc-PuVI-pLZXRZXAms,btNwRKLctHArefil3\n",
      "TIz3:Ryyy$YNwgm;Uquzk!K,DhjX'tLitnwhgs'tTy$H,S''SV:Fb\n",
      ":Unn3v-bubtiutfvkma'cIdraEN&O\n",
      "cwon&g,MUE&q&.\n",
      "u?JLahRbAer.tPGnTYYGISXlqal?\n",
      "P$luj\n",
      "EOlO:wKyaMPdqolTx$B.vJKzbaDcady$I.WTIfz?uMNVUhV,!PkVn'I YNHFfgD'?JucYf.wmZ?Xm&cX$&zFiETMe,O;x:!BEEIYt,uj?\n",
      "$MGcilYjdk!GkCWJkNJUGuSU f b.OErA:jV&WIqoulfyXTBn$ t&qoB,mbL\n",
      "ywlBMr'bin\n",
      "L&wYAHXyRKnGzyvMelMuSJ!M.PhtlLau?PifWC;-nDRLJy,l:XvuwLC!KmdrkKRWzen,cFb?W;jLqpmZ&Nw.RUxJAgIvMR!a-DmfT?rMYTJ\n",
      "hAZYtxH'pHoTLEMc\n",
      "Fs?\n",
      ",WqL.yG3PGy:NQ$V,dtS!nhimy-g:ZzXPMNFcVNaS?UBmKpHL imti$vceadBTaHe LukH3W$JZkFxJfTaT:H?brXhzuGKPfeBvikDsc,p'Liqe,;jPgG-eBYcihxoz,xi;?PDVw\n",
      "Al!hqklCbAce t:d?LH3ghwsDxIHdBbasMKys fVFG\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b2fa41-4a7f-4794-a46f-9a673a480d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18410d1f-329c-4e0c-b29d-e72031715900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
